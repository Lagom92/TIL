{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 머신러닝(분류문제)-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과거 인공지능은 학습과정을 알 수 없는 블랙박스 였다.\n",
    "\n",
    "- 다음에 나오는 알고리즘은 학습에 나온 결과의 이유가 중요할 때 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결정 트리 학습(DEcision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- '설명'이 중요할때 유용하게 사용된다.\n",
    "\n",
    "- 일련의 질문에 대한 결정을 통해 데이터를 분해하는 모델\n",
    "\n",
    "- 훈련 데이터에 있는 변수 즉 특성을 기반으로 새로운 샘플의 클레스 레이블을 추정할 수 있도록 일련의 질문을 학습하게 된다.\n",
    "\n",
    "- 범주형 변수 뿐만 아니라 실수형 변수에서도 사용이 된다.\n",
    "\n",
    "- 트리의 루트(root)에서 시작해서 **정보 이득(Information Gain, IG)**이란 값이 최대가 되는 특성으로 데이터를 나눈다.\n",
    "\n",
    "- 이러한 결정을 리프 노드(leaf node)가 순수해질 때 까지 모든 자식 노드에서 이 분할 작업을 반복한다.\n",
    "\n",
    "- 하지만 분할 작업이 계속되다 보면 깊은 트리가 만들어지게 되는데, 이는 **과적합**을 불러 일으킬 수 있다.\n",
    "\n",
    "- 그래서 일반적으로 트리의 최대 깊이를 제한하여 트리를 **가지치기(pruning)** 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목적함수의 목적\n",
    "\n",
    "- 가장 정보가 풍부한 특성으로 노드를 나누기 위함\n",
    "\n",
    "- 트리 알고리즘으로 최적화\n",
    "\n",
    "- 이 목적함수는 각 분할에서 **정보 이득(IG)을 최대화** 해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수식\n",
    "\n",
    "![](./img/정보이득.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- f = 어떠한 분할 기법을 사용할 것 인가\n",
    "\n",
    "- D_p, D_j = 부모와 j번째 자식 노드의 데이터 셋(p: 부모, j: 자식)\n",
    "\n",
    "- I = 불순도 지표(Impurity)\n",
    "\n",
    "- N_p, N_j = 부모, 자식 노드에 있는 전체 데이터의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 정보 이득 = 부모 노드의 분소도 - 자식 노드의 불순도 합\n",
    "\n",
    "- 자식 노드의 불순도가 낮을수록 정보 이득은 커진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/정보이득최대화.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분의 라이브러리들은 구현을 간단하게 하고, 탐색 공간을 줄이기 위해 **이진 결정 트리**를 사용한다.\n",
    "\n",
    "- 즉, 부모 노드를 두 개의 자식 노드(D_left, D_right)로 나눈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3가지 불순도 지표 혹은 분할 조건\n",
    "\n",
    "1. 지니 불순도(Gini impurity, I_G)\n",
    "\n",
    "2. 엔트로피(entropy, I_H)\n",
    "\n",
    "3. 분류 오차(classification error, I_E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 지니 불순도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수식\n",
    "\n",
    "![](./img/지니불순도.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엔트로피와 반대 개념이라고 생각하면 편하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 잘못 분류될 확률을 최소화 하기 위한 기준"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클래스가 완벽하게 섞여 있을때, 지니 불순도가 최대값이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 지니 불순도와 엔트로피 모두 매우 비슷한 결과를 나타낸다.\n",
    "\n",
    "- 그래서 지니 불순도나 엔트로피를 바꿔가며 트리 평가를 진행하기 보다는 **가지치기** 수준을 바꿔가며 튜닝하는것을 권장한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수식\n",
    "\n",
    "![](./img/엔트로피.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 한 노드의 모든 데이터가 같은 클래스라면 **엔트로피는 0**\n",
    "\n",
    "- 반대로 클래스 분포가 균등하다면 **엔트로피는 최대 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ex) 이진 클래스인 경우 특정 노드 t에서 클래스 i가 1 일 때, 1에 속한 비율이 완전한 1, 또는 0이라면 엔트로피 값은 0이 된다. \n",
    "\n",
    "=> 모든 데이터가 같은 클래스로 분류되기 때문\n",
    "\n",
    "- ex) 반대로 특정 노드 t에서 클래스 i가 1 또는 0이고, 속한 비율이 각각 0.5 균등하게 분포되어 있다면 엔트로피 값은 1이 된다.\n",
    "\n",
    "=> 모든 데이터가 균등한 클래스로 분류되어 있기 때문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 엔트로피는 트리의 상호 의존 정보를 최대화 하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 분류 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 수식\n",
    "\n",
    "![](./img/분류오차.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 두 클래스가 같은 비율일때 최댓값 0.5를 출력 한다.\n",
    "\n",
    "- ex) (20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 이 지표는 권장 되지 않는다!\n",
    "\n",
    "- 이유: 노드의 클래스 확률 변화에 **둔감** 하기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 공부와 예시를 이용해 지니 불순도 추가 공부하기!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그래프를 그리면 지니 불순도가 엔트로피와 분류 오차의 중간에 위치한것을 알수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 측정 분류 기준에의해 자식 노드를 생성\n",
    "\n",
    "- 반복하여 가지를 뻗어 나간다.\n",
    "\n",
    "- 최종적으로 불순도가 0에 수렴할 때까지 즉, 하나의 클래스만을 가진 노드가 될 때까지 이를 반복해서 진행\n",
    "\n",
    "- 이처럼 무한히 반복할 경우 과적합이 발생할 수 있으므로, 가지가 뻗어 내려가는 정도를 설정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "## K-최근접 이웃(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 훈련 과정을 진행하지 않는 머신러닝이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 일반적인 학습을 진행하는 알고리즘의 경우\n",
    "\n",
    "    - 과정: 데이터 정제, 모델 학습 과정 진행, 최종 하이퍼파라미터 수정 등.. 그리고 학습 데이터 제거(데이터 저장 공간 불필요)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KNN 알고리즘의 경우\n",
    "\n",
    "    - 학습이란 개념이 없다.\n",
    "\n",
    "    - 매번 알고리즘을 실행할 때마다 모든 학습데이터를 통해 분류를 진행한다.\n",
    "    \n",
    "    - 즉, 매번 실행할 때 마다 학습 데이터가 필요하다.\n",
    "\n",
    "    - 만약 고차원의 데이터로 실험을 진행한다면, 계산 복잡도 또한 훈련 데이터의 개수에 비례하여 증가하는 경우도 발생한다.\n",
    "\n",
    "    - 그럼에도, KNN은 데이터 정제만 잘해준다면 학습 과정이 없으므로 빠르게 결과를 살펴볼 수 있다는 장점이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN 과정\n",
    "\n",
    "1. 숫자 k와 거리 측정 기준을 선택한다.\n",
    "\n",
    "    - 일반적으로 거리 측정 기준은 유클리디안 거리 측정 방식을 사용한다.\n",
    "\n",
    "2. 분류하려는 미지의 데이터에서 k개의 최근접 이웃을 찾는다.\n",
    "\n",
    "3. 다수결 투표를 진행, 투표 결과에 따라 미지의 데이터 클래스 레이블을 할당한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 알고리즘의 핵심\n",
    "\n",
    "    - 과적합과 과소 적합 사이에서 올바른 균형을 잡기 위한 k값 설정이 중요하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 차원의 저주\n",
    "\n",
    "    - 고정된 크기의 훈련 데이터셋 차원이 늘어남에 따라 특성 공간이 점점 희소해 지는 현상\n",
    "\n",
    "- 차원의 저주를 해소하기 위해서 아래 방법을 사용\n",
    "\n",
    "    - 올바른 변수의 선택\n",
    "\n",
    "    - 차원 축소 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "--------------\n",
    "\n",
    "### 퀴즈\n",
    "\n",
    "1. 결정 트리 학습(Decision tree)에서는 노드(node)를 _ _ _ _이 최대가 되는 특성으로 데이터를 나누게 된다. (4글자 - 띄어쓰기 없이 입력해주세요)\n",
    "    \n",
    "    답: 정보이득\n",
    "\n",
    "<br/>\n",
    "\n",
    "2. 결정 트리 학습(Decision tree)에서 너무 많은 분기로 인해 과적합 되는 걸 막기 위해 분기를 재조정하는 방법을 무엇이라 하는가? (4글자 - 띄어쓰기 없이 입력해주세요)\n",
    "    \n",
    "    답: 가지치기\n",
    "\n",
    "<br/>\n",
    "\n",
    "3. KNN 알고리즘에 대한 사실로 올바르지 않은 것은?\n",
    " \n",
    "    1. 최적의 K를 선택하기 어렵다.\n",
    " \n",
    "    2. 고차원의 데이터나 데이터 수가 많아질수록 계산 비용이 높아진다.\n",
    " \n",
    "    3. 충분한 알고리즘 학습 후 사용 가능하다.\n",
    " \n",
    "    4. 학습 유형은 지도학습에 해당한다\n",
    "\n",
    "    답: 3\n",
    "\n",
    "<br/>\n",
    "\n",
    "4. 고정된 크기의 훈련 데이터셋 차원이 늘어남에 따라 특성 공간이 점점 희소해 지는 현상을 무엇이라 하는가? (5글자 - 띄어쓰기 없이 입력해주세요)\n",
    "    \n",
    "    답: 차원의저주"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}