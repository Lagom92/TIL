# Lecture 10-1

## ReLU: Better non-linearity



<br/>

### NN for XOR

#### Activation function

- sigmoid



<br/>

 ### lec9-2: Backpropagation(Chain rule)



<br/>

### Vanishing gradient

![](./img/vanishing_gradient.png)



<br/>

### ReLU



<br/>

![](./img/activation_function.png)



<br/>

