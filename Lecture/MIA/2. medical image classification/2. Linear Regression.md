# 2. Linear Regression

<br/>

## Linear Regression

<br/>
$$
h_{w, b}(x) = wx + b
$$


<br/>
$$
J(w, b) = \frac{1}{2m}\sum_{i=1}^m (h_{w, b}(x^{(i)} - y^{(i)})^2
$$


<br/>
$$
minimize j(w, b)
$$
w, b 를 찾는 것



<br/>

![](./img/linear_regression.png)





<br/>

## Cost Function

<br/>
$$
J(w, b) = \frac{1}{2m}\sum_{i=1}^m (h_{w, b}(x^{(i)} - y^{(i)})^2
$$




## Gradient Descent

<br/>
$$
b := b - \alpha \frac{\partial}{\partial b} J(w, b)
$$
<br/>
$$
w := w - \alpha \frac{\partial}{\partial w} J(w, b)
$$
<br/>
$$
w_j := w_j - \alpha \frac{\partial}{\partial w_j} J
$$


<br/>

![](./img/gradient_descent.png)



<br/>









<br/><br/><br/>