{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow로 선형회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "from elice_utils import EliceUtils\n",
    "elice_utils = EliceUtils()\n",
    "\n",
    "# 채점을 위해 랜덤 시드를 고정하는 코드입니다.\n",
    "# 정확한 채점을 위해 코드를 변경하지 마세요!\n",
    "np.random.seed(100)\n",
    "\n",
    "# 선형 회귀 클래스 구현\n",
    "class LinearModel:\n",
    "    def __init__(self):\n",
    "        # 1. 가중치 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.\n",
    "        self.W = tf.Variable(1.5)\n",
    "        \n",
    "        # 1. 편향 초기값을 1.5의 값을 가진 변수 텐서로 설정하세요.\n",
    "        self.b = tf.Variable(1.5)\n",
    "        \n",
    "    def __call__(self, X, Y):\n",
    "        # 2. W, X, b를 사용해 선형 모델을 구현하세요.\n",
    "        return tf.add(tf.multiply(self.W, X),self.b) \n",
    "    \n",
    "# 3. MSE 값으로 정의된 loss 함수 선언 \n",
    "def loss(y, pred):\n",
    "    return tf.reduce_mean(tf.square(y - pred))\n",
    "\n",
    "# gradient descent 방식으로 학습 함수 선언\n",
    "def train(linear_model, x, y):\n",
    "\n",
    "    with tf.GradientTape() as t:\n",
    "        current_loss = loss(y, linear_model(x, y))\n",
    "    \n",
    "    # learning_rate 값 선언\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    # gradient 값 계산\n",
    "    delta_W, delta_b = t.gradient(current_loss, [linear_model.W, linear_model.b])\n",
    "    \n",
    "    # learning rate와 계산한 gradient 값을 이용하여 업데이트할 파라미터 변화 값 계산 \n",
    "    W_update = (learning_rate * delta_W)\n",
    "    b_update = (learning_rate * delta_b)\n",
    "    \n",
    "    return W_update,b_update\n",
    " \n",
    "def main():\n",
    "    # 데이터 생성\n",
    "    x_data = np.linspace(0, 10, 50)\n",
    "    y_data = 4 * x_data + np.random.randn(*x_data.shape)*4 + 3\n",
    "\n",
    "    # 데이터 출력\n",
    "    plt.scatter(x_data,y_data)\n",
    "    plt.savefig('data.png')\n",
    "    elice_utils.send_image('data.png')\n",
    "\n",
    "    # 선형 함수 적용\n",
    "    linear_model = LinearModel()\n",
    "\n",
    "    # epochs 값 선언\n",
    "    epochs = 100\n",
    "\n",
    "    # epoch 값만큼 모델 학습\n",
    "    for epoch_count in range(epochs):\n",
    "\n",
    "        # 선형 모델의 예측 값 저장\n",
    "        y_pred_data=linear_model(x_data, y_data)\n",
    "\n",
    "        # 예측 값과 실제 데이터 값과의 loss 함수 값 저장\n",
    "        real_loss = loss(y_data, linear_model(x_data, y_data))\n",
    "\n",
    "        # 현재의 선형 모델을 사용하여  loss 값을 줄이는 새로운 파라미터로 갱신할 파라미터 변화 값을 계산\n",
    "        update_W, update_b = train(linear_model, x_data, y_data)\n",
    "        \n",
    "        # 선형 모델의 가중치와 편향을 업데이트합니다. \n",
    "        linear_model.W.assign_sub(update_W)\n",
    "        linear_model.b.assign_sub(update_b)\n",
    "\n",
    "        # 20번 마다 출력 (조건문 변경 가능)\n",
    "        if (epoch_count%20==0):\n",
    "            print(f\"Epoch count {epoch_count}: Loss value: {real_loss.numpy()}\")\n",
    "            print('W: {}, b: {}'.format(linear_model.W.numpy(), linear_model.b.numpy()))\n",
    "\n",
    "            fig = plt.figure()\n",
    "            ax1 = fig.add_subplot(111)\n",
    "            ax1.scatter(x_data,y_data)\n",
    "            ax1.plot(x_data,y_pred_data, color='red')\n",
    "            plt.savefig('prediction.png')\n",
    "            elice_utils.send_image('prediction.png')\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Reference\n",
    "\n",
    "- 인강 "
   ]
  }
 ]
}